{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('./graph_series/')\n",
    "path = './graph_series/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens():\n",
    "    tokens = {}\n",
    "    with open('./nodes-SG.json') as f:\n",
    "        tokens = json.load(f)\n",
    "    return tokens\n",
    "\n",
    "def getInputSeries():\n",
    "    tokens = getTokens()\n",
    "    nodes_name = list(tokens.keys())\n",
    "    nodes_int = [tokens[item] for item in tokens.keys()]\n",
    "    one_hot = np.eye(len(tokens.keys()))\n",
    "    inputSeries = []\n",
    "    allSeries = []\n",
    "    i = 1\n",
    "    for file in tqdm(files):\n",
    "        vec = np.loadtxt(path+file,dtype=np.int32)\n",
    "#         allSeries.extend(vec.tolist())\n",
    "#         if vec.shape[0] > 300:\n",
    "#             vec = vec[:300]\n",
    "#         else:\n",
    "#             vec = np.append(vec,np.zeros(300-vec.shape[0]))\n",
    "        inputSeries.append(vec.tolist())\n",
    "    return inputSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171019/171019 [26:23<00:00, 107.98it/s]\n"
     ]
    }
   ],
   "source": [
    "inputSeries = getInputSeries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25683649\n"
     ]
    }
   ],
   "source": [
    "counter = collections.Counter([tk for st in inputSeries for tk in st])\n",
    "counter = dict(filter(lambda x: x[1] >= 0, counter.items()))\n",
    "\n",
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx] for st in inputSeries]\n",
    "num_tokens = sum([len(st) for st in dataset])\n",
    "\n",
    "print(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529,\n",
       "       530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542,\n",
       "       543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555,\n",
       "       556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568,\n",
       "       569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581,\n",
       "       582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594,\n",
       "       595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607,\n",
       "       608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620,\n",
       "       621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633,\n",
       "       634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646,\n",
       "       647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659,\n",
       "       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n",
       "       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
       "       686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698,\n",
       "       699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711,\n",
       "       712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724,\n",
       "       725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737,\n",
       "       738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750,\n",
       "       751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763,\n",
       "       764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776,\n",
       "       777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789,\n",
       "       790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(list(counter.keys()),dtype=np.int32)\n",
    "a = np.sort(a,axis=0,kind='quicksort',order=None)\n",
    "li = np.array(range(801))\n",
    "np.setdiff1d(li,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2149103\n"
     ]
    }
   ],
   "source": [
    "def discard(idx):\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "\n",
    "print(sum([len(st) for st in subsampled_dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2: # 至少两个词才能组成中心词\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size),\n",
    "            min(len(st), center_i + 1 + window_size)))\n",
    "            indices.remove(center_i) # 去除中心词\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                 i, neg_candidates = 0, random.choices(population, sampling_weights, k=int(1e5))\n",
    "            neg, i = neg_candidates[i], i + 1\n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "sampling_weights = [counter[w]**0.75 for w in idx_to_token]\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "         return (self.centers[index], self.contexts[index],self.negatives[index])\n",
    "         \n",
    "    def __len__(self):\n",
    "         return len(self.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (torch.tensor(centers).view(-1, 1),\n",
    "             \n",
    "torch.tensor(contexts_negatives),\n",
    "torch.tensor(masks), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2135328, 2135328, 2135328)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_centers), len(all_contexts), len(all_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "dataset = MyDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "collate_fn=batchify,num_workers=num_workers)\n",
    "\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks', 'labels'], batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4525,  1.9874, -0.9452,  ..., -0.7464,  0.9073,  0.1377],\n",
       "        [-0.9171, -0.8009, -0.6175,  ..., -1.0223,  1.0705, -0.4404],\n",
       "        [ 0.9239, -0.3658,  0.2486,  ..., -0.7920,  0.8907,  0.3150],\n",
       "        ...,\n",
       "        [ 0.0952, -1.0623,  0.0595,  ...,  0.0861, -1.1065,  0.7058],\n",
       "        [-0.7767,  1.0824, -0.0778,  ..., -0.7923, -0.2387,  0.3701],\n",
       "        [-0.5217,  0.6557,  0.1799,  ..., -0.1496,  0.8289,  0.7726]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = nn.Embedding(num_embeddings=801, embedding_dim=100)\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self): # none mean sum\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "#          \"\"\"\n",
    "#          input – Tensor shape: (batch_size, len)\n",
    "#          target – Tensor of the same shape as input\n",
    "#          \"\"\"\n",
    "        inputs, targets, mask = inputs.float(), targets.float(),mask.float()\n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "        return res.mean(dim=1)\n",
    "\n",
    "loss = SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8740\n",
      "1.2100\n"
     ]
    }
   ],
   "source": [
    "def sigmd(x):\n",
    "    return - math.log(1 / (1 + math.exp(-x)))\n",
    "\n",
    "print('%.4f' % ((sigmd(1.5) + sigmd(-0.3) + sigmd(1) + sigmd(-2)) / 4)) \n",
    "print('%.4f' % ((sigmd(1.1) + sigmd(-0.6) + sigmd(-2.2)) / 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, you could change the feautre size of per node.\n",
    "embed_size = 100 \n",
    "net = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n",
    "    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, lr, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter: \n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "\n",
    "            l = (loss(pred.view(label.shape), label, mask) *\n",
    "            mask.shape[1] / mask.float().sum(dim=1)).mean() \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.item()\n",
    "            n += 1\n",
    "        print('epoch %d, loss %.2f, time %.2fs' % (epoch + 1, l_sum / n, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on cuda\n",
      "epoch 1, loss 0.42, time 11.00s\n",
      "epoch 2, loss 0.39, time 10.36s\n",
      "epoch 3, loss 0.39, time 10.02s\n",
      "epoch 4, loss 0.39, time 9.81s\n",
      "epoch 5, loss 0.39, time 9.81s\n",
      "epoch 6, loss 0.39, time 9.88s\n",
      "epoch 7, loss 0.39, time 9.86s\n",
      "epoch 8, loss 0.39, time 9.85s\n",
      "epoch 9, loss 0.39, time 9.94s\n",
      "epoch 10, loss 0.39, time 10.16s\n",
      "epoch 11, loss 0.39, time 10.23s\n",
      "epoch 12, loss 0.39, time 10.23s\n",
      "epoch 13, loss 0.39, time 10.26s\n",
      "epoch 14, loss 0.39, time 10.12s\n",
      "epoch 15, loss 0.39, time 10.38s\n"
     ]
    }
   ],
   "source": [
    "train(net, 0.01, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = net[0].weight.data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.array(weight,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./word_vectors_SG.txt',weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,  10,   4,   5,   6,   7,   8,   9,  17,  18,\n",
       "        13,  14,  20,  21,  16,  82,  23,  24,  45,  46,  86,  11,  12,\n",
       "        19,  15,  85,  89,  88,  47,  35,  25,  28, 101,  90,  27,  29,\n",
       "        75,  34, 112, 106,  91,  26,  36,  48,  52,  50,  53,  80,  81,\n",
       "        84,  92,  93,  94,  95,  98,  96, 100,  30,  31,  79, 102,  22,\n",
       "       103,  37,  83,  55,  87,  97, 138, 142,  38,  39,  99,  40, 135,\n",
       "        41,  42,  43,  44, 121, 122, 123,  49,  51,  54,  56,  57,  58,\n",
       "        59,  60, 115,  61,  32,  33, 114, 147,  62,  63,  64,  65,  66,\n",
       "        67,  68,  69,  70,  71,  72,  73,  74,  76,  77,  78, 118, 119,\n",
       "       116, 140, 139, 148, 150, 111, 134, 143, 283, 141, 104, 105, 107,\n",
       "       108, 109, 110, 117, 124, 224, 144, 120, 125, 126, 128, 129, 151,\n",
       "       152, 153, 154, 155, 156, 157, 158, 166, 167, 168, 127, 163, 188,\n",
       "       189, 212, 213, 225, 164, 131, 146, 291, 214, 217, 218, 222, 223,\n",
       "       227, 228, 229, 230, 231, 232, 233, 145, 136, 137, 113, 190, 130,\n",
       "       133, 162, 216, 280, 234, 235, 236, 237, 238, 239, 149, 371, 279,\n",
       "       364, 409, 191, 366, 367, 368, 199, 169, 170, 171, 172, 173, 174,\n",
       "       175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 193,\n",
       "       194, 195, 196, 197, 198, 200, 201, 202, 398, 159, 219, 288, 299,\n",
       "       300, 161, 278, 393, 298, 395, 303, 240, 241, 242, 243, 244, 245,\n",
       "       246, 247, 248, 249, 250, 251, 285, 252, 253, 254, 255, 256, 257,\n",
       "       258, 259, 260, 261, 302, 361, 221, 293, 215, 132, 394, 262, 304,\n",
       "       410, 370, 411, 263, 264, 265, 266, 307, 308, 309, 310, 311, 312,\n",
       "       313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "       326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338,\n",
       "       339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351,\n",
       "       352, 353, 354, 355, 356, 357, 358, 359, 407, 360, 406, 292, 402,\n",
       "       301, 287, 226, 306, 220, 203, 204, 205, 206, 207, 208, 209, 210,\n",
       "       211, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 373,\n",
       "       374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386,\n",
       "       387, 388, 389, 390, 391, 515, 516, 405, 392, 365, 514, 400, 187,\n",
       "       397, 413, 396, 369, 363, 414, 415, 416, 417, 418, 419, 420, 421,\n",
       "       422, 423, 424, 425, 426, 427, 281, 372, 160, 289, 362, 403, 165,\n",
       "       290, 286, 401, 305, 399, 412, 282, 513, 404, 428, 429, 430, 431,\n",
       "       432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444,\n",
       "       445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457,\n",
       "       458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470,\n",
       "       471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483,\n",
       "       484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496,\n",
       "       497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
       "       510, 511, 294, 295, 296, 297, 408, 192, 284, 512])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./word_index_SG.txt',np.array(idx_to_token,dtype=np.int32),fmt = \"%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_to_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
