{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score,multilabel_confusion_matrix,roc_curve,roc_auc_score\n",
    "from pytorchtools import EarlyStopping  #location: /utils/\n",
    "from utils import sparse_mx_to_torch_sparse_tensor,accuracy\n",
    "from layers import GraphConvolution, GraphAttention\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    def __init__(self,num_nodes,edge_pairs,features,label,node_degs):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.edge_pairs = edge_pairs\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.node_degs = node_degs  #邻接点数量\n",
    "        \n",
    "    def __str__(self):\n",
    "        return ('nodes: %d  edge_pairs: %d features: %d' % (self.num_nodes,len(self.edge_pairs),len(self.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGraph(filename):\n",
    "    f = open(filename)\n",
    "    row = f.readline().strip().split()\n",
    "    nodes,label = [int(w) for w in row]\n",
    "    node_features = []\n",
    "    edge_pairs = []\n",
    "    node_degs = []\n",
    "    for j in range(nodes):\n",
    "        row = f.readline().strip().split()\n",
    "        node_deg = int(row[0])+1\n",
    "        row ,attr = [int(w) for w in row[1:int(row[0])+1]],np.array([float(w) for w in row[int(row[0])+1:]])\n",
    "        if attr is not None:\n",
    "            node_features.append(attr)\n",
    "        if node_deg is not None:\n",
    "            node_degs.append(node_deg)\n",
    "        if row is not None:\n",
    "            for k in row:\n",
    "                edge = [j,k]\n",
    "                edge_pairs.append(edge)\n",
    "    g = Graph(nodes,edge_pairs,node_features,label,node_degs)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGraphList(files):\n",
    "    glist = []\n",
    "    for file in files:\n",
    "        graph = getGraph(file)\n",
    "        glist.append(graph)\n",
    "    return glist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEdgeList(files,k):\n",
    "    EdgeList = []\n",
    "    for file in files:\n",
    "        li = np.loadtxt(file)\n",
    "        EdgeList.append(li)\n",
    "    EdgeList = processEdgeList(EdgeList,k)\n",
    "    return EdgeList\n",
    "\n",
    "def processEdgeList(edges,k):\n",
    "    newList = []\n",
    "    for item in edges:\n",
    "        if item.shape[0] >= k:\n",
    "            newList.append(item[:k,:].tolist())\n",
    "        else:\n",
    "            t = np.zeros((k-item.shape[0],100))\n",
    "            newList.append(np.concatenate((item,t)).tolist())\n",
    "    return torch.FloatTensor(np.array(newList))\n",
    "\n",
    "\n",
    "def getEdgeNum(files):\n",
    "    EdgeNum = []\n",
    "    for file in files:\n",
    "        li = np.loadtxt(file)\n",
    "        EdgeNum.append(li.shape[0])\n",
    "    return EdgeNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# te = getEdgeList(train_edge_data[:5],99)\n",
    "# te = torch.FloatTensor(te)\n",
    "# mo = Edge_Attention(2,100)\n",
    "# a = mo(te)\n",
    "# encoder_layer = nn.TransformerEncoderLayer(d_model=100, nhead=2)\n",
    "# src = torch.rand(10, 32, 512)\n",
    "# out = encoder_layer(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [1,2,3,4]\n",
    "# b = [1,2,6,4]\n",
    "# (np.array(a)+np.array(b))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_Graph(graph_list):\n",
    "    prefix_sum = []\n",
    "    node_features = []\n",
    "    node_degs = []\n",
    "    node_labels = []\n",
    "    total_num_edges = 0\n",
    "    total_num_nodes = 0\n",
    "    edge_pairs = []\n",
    "    graph_sizes = []\n",
    "    for i in range(len(graph_list)):\n",
    "        prefix_sum.append(graph_list[i].num_nodes)\n",
    "        if i != 0:\n",
    "            prefix_sum[i] += prefix_sum[i-1]\n",
    "        node_features.extend(graph_list[i].features)\n",
    "        node_degs.extend(graph_list[i].node_degs)\n",
    "        node_labels.append(graph_list[i].label)\n",
    "        total_num_edges += len(graph_list[i].edge_pairs)\n",
    "        total_num_nodes += graph_list[i].num_nodes\n",
    "        graph_sizes.append(graph_list[i].num_nodes)\n",
    "        edge_pairs.append(graph_list[i].edge_pairs)\n",
    "    # create batch_graph\n",
    "    n2n_idxes = torch.LongTensor(2, total_num_edges)\n",
    "    n2n_vals = torch.FloatTensor(total_num_edges)\n",
    "    \n",
    "    for i in range(len(graph_list)):\n",
    "        prefix_sum[len(graph_list)-i-1] = prefix_sum[len(graph_list)-i-2]\n",
    "    prefix_sum[0] = 0\n",
    "    \n",
    "    for i in range(total_num_edges):\n",
    "        n2n_vals[i] = 1\n",
    "    \n",
    "    j = 0\n",
    "    for i in range(len(graph_list)):\n",
    "        for item in edge_pairs[i]:\n",
    "            n2n_idxes[0][j] = item[0]+prefix_sum[i]\n",
    "            n2n_idxes[1][j] = item[1]+prefix_sum[i]\n",
    "            \n",
    "#             if item[0]+prefix_sum[i] > total_num_nodes:\n",
    "#                 print('item0',item[0],prefix_sum[i],total_num_nodes)\n",
    "#             if item[1]+prefix_sum[i] > total_num_nodes:\n",
    "#                 print('item1',item[1],prefix_sum[i],total_num_nodes)\n",
    "                \n",
    "#             if item[0]+prefix_sum[i] < 0:\n",
    "#                 print('item0',item[0],prefix_sum[i],'position: ',i)\n",
    "#             if item[1]+prefix_sum[i] < 0 :\n",
    "#                 print('item1',item[1],prefix_sum[i],'position: ',i)\n",
    "                \n",
    "            j += 1\n",
    "#     print(j,total_num_edges)    \n",
    "#     print(n2n_idxes[:,3000:])\n",
    "#     print(node_features)\n",
    "    n2n = torch.sparse.FloatTensor(n2n_idxes, n2n_vals, torch.Size([total_num_nodes, total_num_nodes]))\n",
    "    node_features = torch.FloatTensor(node_features)\n",
    "    node_degs = 1/torch.LongTensor(node_degs)    \n",
    "    degs_index = torch.LongTensor(2, total_num_nodes)\n",
    "    \n",
    "    for i in range(total_num_nodes):\n",
    "        degs_index[0,i] = i\n",
    "        degs_index[1,i] = i\n",
    "    node_degs = torch.sparse.FloatTensor(degs_index, node_degs, torch.Size([total_num_nodes, total_num_nodes]))\n",
    "        \n",
    "    return n2n,node_features,node_degs,graph_sizes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInverse(adjs):\n",
    "    Dres = []\n",
    "    for a in adjs:\n",
    "        length = a.size()[0]\n",
    "        D = np.zeros((length,length))\n",
    "        for item,i in zip(a,range(length)):\n",
    "            if item.sum() != 0:\n",
    "                D[i,i] = 1/item.sum()\n",
    "        Dres.append(D)\n",
    "    return torch.tensor(np.array(Dres,dtype = np.float32)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, activation = None, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.activation = activation\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, features, adj, degs):\n",
    "        support = torch.mm(features, self.weight)\n",
    "        output = torch.mm(adj, support)\n",
    "        output = torch.mm(degs,output)\n",
    "        if self.activation != None:\n",
    "            output = self.activation(output)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Edge_Attention(nn.Module):\n",
    "    def __init__(self, head, feature_length):\n",
    "        super(Edge_Attention,self).__init__()\n",
    "        self.head = head\n",
    "        self.feature_length = feature_length\n",
    "        self.Attention = nn.TransformerEncoderLayer(d_model=feature_length, nhead=head, batch_first=True)\n",
    "        \n",
    "    def forward(self,edges):\n",
    "        return self.Attention(edges)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# te = getGraphList(train_graph_data[:5])\n",
    "# n2n,node_features,node_degs,graph_sizes = merge_Graph(te)\n",
    "# mo = res_GCN(100,32,1,5,99)\n",
    "# temp = mo(node_features,n2n,node_degs,graph_sizes)\n",
    "# temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class res_GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, n_layers, k):\n",
    "        super(res_GCN, self).__init__()\n",
    "\n",
    "        self.k = k\n",
    "        self.n_layers = n_layers\n",
    "        self.nhid = nhid\n",
    "        self.nclass = nclass\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.total_latent_dim = nhid * n_layers + nclass\n",
    "        # 输入层\n",
    "        self.layers.append( GraphConvolution( nfeat, nhid, activation = torch.tanh ))\n",
    "        # 隐层\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GraphConvolution(nhid, nhid, activation = torch.tanh ))\n",
    "        # 输出层\n",
    "        self.layers.append( GraphConvolution(nhid, nclass))\n",
    "        \n",
    "        \n",
    "    def forward(self, features, graphs,degs,graph_sizes):\n",
    "        \n",
    "        # def a res GCNN here\n",
    "        feature_list = []\n",
    "        for i, layer in enumerate( self.layers):\n",
    "            if i!= 0 and i != len(self.layers)-1:\n",
    "                features = layer( features, graphs, degs) + features\n",
    "                feature_list.append(features)\n",
    "            else:\n",
    "                features = layer( features, graphs, degs)\n",
    "                feature_list.append(features)\n",
    "#         sort pooling with k row remains\n",
    "\n",
    "        res = ''\n",
    "        for i,item in enumerate(feature_list):\n",
    "            if i == 0:\n",
    "                res = item\n",
    "            else:\n",
    "                res = torch.cat((res,item),1)\n",
    "                \n",
    "#         res = features\n",
    "\n",
    "#         print(len(feature_list),res.shape)\n",
    "        sort_channel = res[:, -1]\n",
    "        batch_sortpooling_graphs = torch.zeros(len(graph_sizes), self.k, self.total_latent_dim)\n",
    "        if torch.cuda.is_available() and isinstance(features.data, torch.cuda.FloatTensor):\n",
    "            batch_sortpooling_graphs = batch_sortpooling_graphs.cuda()\n",
    "\n",
    "        batch_sortpooling_graphs = Variable(batch_sortpooling_graphs)\n",
    "        accum_count = 0\n",
    "        for i in range(len(graph_sizes)):\n",
    "            to_sort = sort_channel[accum_count: accum_count + graph_sizes[i]]\n",
    "            k = self.k if self.k <= graph_sizes[i] else graph_sizes[i]\n",
    "            _, topk_indices = to_sort.topk(k)\n",
    "            topk_indices += accum_count\n",
    "            sortpooling_graph = res.index_select(0, topk_indices)\n",
    "            if k < self.k:\n",
    "                to_pad = torch.zeros(self.k-k, self.total_latent_dim)\n",
    "                if torch.cuda.is_available() and isinstance(features.data, torch.cuda.FloatTensor):\n",
    "                    to_pad = to_pad.cuda()\n",
    "\n",
    "                to_pad = Variable(to_pad)\n",
    "                sortpooling_graph = torch.cat((sortpooling_graph, to_pad), 0)\n",
    "            batch_sortpooling_graphs[i] = sortpooling_graph\n",
    "            accum_count += graph_sizes[i]\n",
    "\n",
    "        return batch_sortpooling_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self,classNum,dropout_rate,nfeat,nhid,nclass,n_layers,k,head,features_length):\n",
    "        super(Classifier,self).__init__()\n",
    "        self.classNum = classNum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.resGCNN = res_GCN(nfeat,nhid,nclass,n_layers,k)\n",
    "        self.edgeAttention = Edge_Attention(head,features_length)\n",
    "        \n",
    "#         self.layer_norm1 = nn.LayerNorm()\n",
    "        self.layer_norm2 = nn.LayerNorm(features_length)\n",
    "        self.cov1 = nn.Conv1d(in_channels=2, out_channels=16,kernel_size = nhid*n_layers+nclass,stride = nhid*n_layers+nclass)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size = 2, stride= 2)\n",
    "        self.dropout1 = nn.Dropout(p = self.dropout_rate)\n",
    "        self.cov2 = nn.Conv1d(in_channels=16, out_channels=32,kernel_size = 4,stride = 4)\n",
    "        self.dropout2 = nn.Dropout(p = self.dropout_rate)\n",
    "       \n",
    "        dense = int(k/2/4)\n",
    "        \n",
    "        self.denseLayer1 = nn.Linear(dense*32,512)\n",
    "        self.dropout3 = nn.Dropout(p = self.dropout_rate)\n",
    "        self.denseLayer2 = nn.Linear(512,128)\n",
    "        self.dropout4 = nn.Dropout(p = self.dropout_rate)\n",
    "        self.outputLayer = nn.Linear(128,classNum)\n",
    "        \n",
    "    def forward(self,features, graphs,degs,graph_sizes,edges):\n",
    "        gcn = self.resGCNN(features,graphs,degs,graph_sizes)\n",
    "#         gcn = self.layer_norm1(gcn)\n",
    "        e_attention = self.edgeAttention(edges)\n",
    "        e_attention = self.layer_norm2(e_attention)\n",
    "#         print(gcn.shape,e_attention.shape)\n",
    "        \n",
    "        gcn = gcn.view(len(graph_sizes),1,-1)\n",
    "        e_attention = e_attention.view(len(graph_sizes),1,-1)\n",
    "        \n",
    "        \n",
    "#         print(,gcn.shape,e_attention.shape)\n",
    "        res = torch.cat((gcn,e_attention),1)\n",
    "        \n",
    "#         res = res.view(len(graph_sizes),1,-1)\n",
    "#        1d convolution layer\n",
    "        res = F.relu(self.cov1(res))\n",
    "        res = self.dropout1(res)\n",
    "        res = self.maxpool(res)\n",
    "        res = F.relu(self.cov2(res))\n",
    "        res = F.relu(res)\n",
    "        res = self.dropout2(res)\n",
    "#         Dense Layer \n",
    "        res = res.view(len(graph_sizes),-1)\n",
    "        res = F.relu(self.denseLayer1(res))\n",
    "        res = self.dropout3(res)\n",
    "        res = F.relu(self.denseLayer2(res))\n",
    "        res = self.dropout4(res)\n",
    "        output = self.outputLayer(res)\n",
    "        return output.flatten()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.tensor([[0,1],[1,0]])\n",
    "# a = a.unsqueeze(0)\n",
    "# a.repeat((3,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = torch.randn(801,100)\n",
    "# a = torch.tensor([[-0.9950, -0.6175, -0.1253,  1.3536],\n",
    "#         [ 0.1208, -0.4237, -1.1313,  0.9022],\n",
    "#         [-1.1995, -0.0699, -0.4396,  1.999]])\n",
    "# # a,_  = torch.sort(a,dim=0,descending=True)\n",
    "# key = [row[-1].item() for row in res]\n",
    "# key = np.array(key)\n",
    "# key = np.argsort(key,axis=-1,kind = 'quicksort')\n",
    "# key = np.flipud(key)\n",
    "# res = [list(res[i]) for i in key[:5]]\n",
    "# res = torch.tensor(res)\n",
    "# print(res.size())\n",
    "# res = res.view(-1,1,5*100)\n",
    "# print(res.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 400, 400)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this model process one graph per time\n",
    "# have a try \n",
    "# 170000 files  shuffle and split into train test set\n",
    "path = '../data/SemanticGraph/Graph_normal/'        # 无漏洞\n",
    "pathA = '../data/SemanticGraph/Graph_vulnerable/'    # 有漏洞\n",
    "files = os.listdir(path)\n",
    "filesA = os.listdir(pathA)\n",
    "\n",
    "e_path = '../data/SemanticGraph/BFS_Edges/Edge_no_v/'\n",
    "e_pathA = '../data/SemanticGraph/BFS_Edges/Edge_v/'\n",
    "\n",
    "edges = os.listdir(e_path)\n",
    "edgesA = os.listdir(e_pathA)\n",
    "\n",
    "graph_filename = [path+file for file in files]\n",
    "graph_filename.extend([pathA+file for file in filesA])\n",
    "\n",
    "edge_filename = [e_path+file for file in edges]\n",
    "edge_filename.extend([e_pathA+file for file in edgesA])\n",
    "\n",
    "all_label = [0.0 for i in range(len(files))]\n",
    "all_label.extend([1.0 for i in range(len(filesA))]) \n",
    "length = len(files) + len(filesA)\n",
    "\n",
    "# 打乱数据\n",
    "from sklearn.utils import shuffle  \n",
    "graph_filename,edge_filename,all_label = shuffle(graph_filename,edge_filename,all_label)\n",
    "\n",
    "#划分数据集\n",
    "k = 0.8\n",
    "k1 = 0.9\n",
    "train_graph_data = graph_filename[0:int(k*length)]\n",
    "train_label = all_label[0:int(k*length)]\n",
    "train_edge_data = edge_filename[0:int(k*length)]\n",
    "\n",
    "valid_graph_data = graph_filename[int(k*length):int(k1*length)]\n",
    "valid_label = all_label[int(k*length):int(k1*length)]\n",
    "valid_edge_data = edge_filename[int(k*length):int(k1*length)]\n",
    "\n",
    "test_graph_data = graph_filename[int(k1*length):]\n",
    "test_label = all_label[int(k1*length):]\n",
    "test_edge_data = edge_filename[int(k1*length):]\n",
    "\n",
    "len(train_graph_data),len(valid_graph_data),len(test_graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getGraphList(files):\n",
    "#     glist = []\n",
    "# #     Myfiles.extend(files)\n",
    "#     for file in files:\n",
    "#         graph = getGraph(file)\n",
    "# #         print(graph)\n",
    "# #         print(file)\n",
    "#         glist.append(graph.num_nodes)\n",
    "#     return glist\n",
    "\n",
    "# a = getGraphList(graph_filename)\n",
    "# b = np.sort(a)\n",
    "# key = int(len(a)*0.9)\n",
    "# print('90%: ',b[key])   # 116\n",
    "# key = int(len(a)*0.8)\n",
    "# print('80%: ',b[key])   # 116\n",
    "# key = int(len(a)*0.6)\n",
    "# print('60%: ',b[key])   # 78\n",
    "\n",
    "# a = getEdgeNum(edge_filename)\n",
    "# b = np.sort(a)\n",
    "# k0 = int(len(a)*0.9) \n",
    "# k1 = int(len(a)*0.8)   \n",
    "# k2 = int(len(a)*0.6)\n",
    "# print('90%: ',b[k0],'80%: ',b[k1],\"60%: \",b[k2])   # 80%:  115 60%:  69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.FloatTensor([[0.5,0.3,0.2],[0.2,0.3,0.8]])\n",
    "# b = torch.FloatTensor([[1,0,0],[0,1,1]])\n",
    "# a = (a >= 0.5).float()\n",
    "# # multilabel_confusion_matrix(b,a)\n",
    "# c = (((a == b).float().sum(dim=1) / 3) == 1).sum()/a.shape[0]\n",
    "# d = (a == b)[:,0].float().sum()\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.tensor([[0.3711, 0.4049, 0.3462],\n",
    "#         [0.3669, 0.4021, 0.3401],\n",
    "#         [0.3930, 0.4236, 0.3792],\n",
    "#         [0.3655, 0.4006, 0.3378],\n",
    "#         [0.3577, 0.3937, 0.3271],\n",
    "#         [0.3572, 0.3936, 0.3267],\n",
    "#         [0.3711, 0.4049, 0.3462],\n",
    "#         [0.3830, 0.4154, 0.3638],\n",
    "#         [0.3887, 0.4202, 0.3726],\n",
    "#         [0.3669, 0.4021, 0.3401],\n",
    "#         [0.3705, 0.4047, 0.3452],\n",
    "#         [0.3617, 0.3973, 0.3337]])\n",
    "# a = a.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predict,t_label,thresh):\n",
    "    pre = (predict >= thresh).float()\n",
    "#     print(predict,t_label)\n",
    "    accuracy = accuracy_score(t_label.data.cpu(),pre.data.cpu())\n",
    "    precision = precision_score(t_label.data.cpu(),pre.data.cpu(), zero_division = 0)\n",
    "    recall = recall_score(t_label.data.cpu(),pre.data.cpu(), zero_division = 0)\n",
    "    f1 = f1_score(t_label.data.cpu(),pre.data.cpu(), zero_division = 0)\n",
    "    return precision, recall, f1, accuracy #without auc\n",
    "\n",
    "def test_evaluate(predict,t_label,thresh):  \n",
    "    pre = (predict >= thresh).float()\n",
    "#     print(predict,t_label)\n",
    "    accuracy = accuracy_score(t_label.data.cpu(),pre.data.cpu())\n",
    "    precision = precision_score(t_label.data.cpu(),pre.data.cpu(), zero_division = 0)\n",
    "    recall = recall_score(t_label.data.cpu(),pre.data.cpu(), zero_division = 0)\n",
    "    f1 = f1_score(t_label.data.cpu(),pre.data.cpu(), zero_division = 0)\n",
    "    # fpr,tpr,_ = roc_curve(t_label.data.cpu(),predict.data.cpu())\n",
    "    auc = roc_auc_score(t_label.data.cpu(),predict.data.cpu())\n",
    "\n",
    "    return precision, recall, f1, accuracy, auc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        graphs,features,node_degs,graph_sizes = merge_Graph(getGraphList(test_graph_data))\n",
    "        graphs = graphs.cuda()\n",
    "        labels = torch.FloatTensor(test_label).cuda()\n",
    "        node_degs = node_degs.cuda()\n",
    "        features = features.cuda()\n",
    "        edges = getEdgeList(test_edge_data,sortk)\n",
    "        edges = edges.cuda()\n",
    "            \n",
    "        graphs = Variable(graphs)\n",
    "        node_degs = Variable(node_degs)\n",
    "        features = Variable(features)\n",
    "        edges = Variable(edges)\n",
    "            \n",
    "        output = model(features,graphs,node_degs,graph_sizes,edges)\n",
    "        l = loss(output,labels)\n",
    "        precesion,recall,f1_score,acc,auc = test_evaluate(torch.sigmoid(output),labels,threshold)\n",
    "\n",
    "    return precesion,recall,f1_score,acc,auc\n",
    "\n",
    "def valid():\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        graphs,features,node_degs,graph_sizes = merge_Graph(getGraphList(valid_graph_data))\n",
    "        graphs = graphs.cuda()\n",
    "        labels = torch.FloatTensor(valid_label).cuda()\n",
    "        node_degs = node_degs.cuda()\n",
    "        features = features.cuda()\n",
    "        edges = getEdgeList(valid_edge_data,sortk)\n",
    "        edges = edges.cuda()\n",
    "                \n",
    "        graphs = Variable(graphs)\n",
    "        node_degs = Variable(node_degs)\n",
    "        features = Variable(features)\n",
    "        edges = Variable(edges)\n",
    "            \n",
    "        output = model(features,graphs,node_degs,graph_sizes,edges)\n",
    "        l = loss(output,labels)\n",
    "        precesion,recall,f1_score,acc = evaluate(torch.sigmoid(output),labels,threshold)\n",
    "\n",
    "    return precesion,recall,f1_score, acc, l.item()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    start = time()\n",
    "    accuracy = []\n",
    "    train_precesion = []\n",
    "    train_recall = []\n",
    "    train_f1 = []\n",
    "    j = 0\n",
    "    global early_stop_flag\n",
    "    for graph_files,edge_files,labels in zip([train_graph_data[i:i + batch_size] for i in range(0, len(train_graph_data), batch_size)],[train_edge_data[i:i + batch_size] for i in range(0, len(train_edge_data), batch_size)],[train_label[i:i + batch_size] for i in range(0, len(train_label), batch_size)]):\n",
    "#         print('-',end = ' ')\n",
    "        j += 1\n",
    "#         print(files,labels)\n",
    "        ss = time()\n",
    "        graphs,features,node_degs,graph_sizes = merge_Graph(getGraphList(graph_files))\n",
    "        graphs = graphs.cuda()\n",
    "        labels = torch.FloatTensor(labels).cuda()\n",
    "        node_degs = node_degs.cuda()\n",
    "        features = features.cuda()\n",
    "        edges = getEdgeList(edge_files,sortk)\n",
    "        edges = edges.cuda()\n",
    "        \n",
    "        graphs = Variable(graphs)\n",
    "        node_degs = Variable(node_degs)\n",
    "        features = Variable(features)\n",
    "        edges = Variable(edges)\n",
    "        \n",
    "        \n",
    "        output = model(features,graphs,node_degs,graph_sizes,edges)\n",
    "        l = loss(output,labels)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "#         for name, parms in model.named_parameters():\t\n",
    "#             print('-->name:', name, '-->grad_requirs:',parms.requires_grad, \\\n",
    "#              ' -->grad_value:',parms.grad)    #查看梯度\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pre,rec,f1, acc = evaluate(output,labels,threshold)\n",
    "        accuracy.append(acc)\n",
    "#         train_precesion.append(pre)\n",
    "#         train_recall.append(rec)\n",
    "#         train_f1.append(f1)\n",
    "        \n",
    "        ee = time()\n",
    "#     train_precesion = np.array(precesion).mean()\n",
    "#     train_recall = np.array(recall).mean()\n",
    "#     train_f1 = np.array(f1_score).mean() \n",
    "    accuracy = np.array(accuracy).mean()\n",
    "    test_precesion,test_recall,test_f1,tacc, tloss = valid()\n",
    "    early_stopping(tloss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        early_stop_flag = True\n",
    "    end = time()\n",
    "    f = open(wf+'records.txt','a') \n",
    "    print('epoch %d, train_loss : %f  test_loss : %f trainacc: %f testacc: %f precesion: %f  recall: %f  f1_score: %f  time: %f' % (epoch+1, l.item(), tloss, accuracy, tacc, test_precesion, test_recall, test_f1, end-start),file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = torch.FloatTensor([0.2,0.6,0.7,0.1,0.3])\n",
    "# label = torch.FloatTensor([1,0,0,0,0])\n",
    "# # a = np.array(output)\n",
    "# # [1.0 if i > 0 else 0.0 for i in output] == label\n",
    "# ((output>0.6).float() == label).float().sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Classifier(\n",
       "  (resGCNN): res_GCN(\n",
       "    (layers): ModuleList(\n",
       "      (0): GraphConvolution (100 -> 11)\n",
       "      (1): GraphConvolution (11 -> 11)\n",
       "      (2): GraphConvolution (11 -> 11)\n",
       "      (3): GraphConvolution (11 -> 11)\n",
       "      (4): GraphConvolution (11 -> 11)\n",
       "      (5): GraphConvolution (11 -> 11)\n",
       "      (6): GraphConvolution (11 -> 11)\n",
       "      (7): GraphConvolution (11 -> 11)\n",
       "      (8): GraphConvolution (11 -> 11)\n",
       "      (9): GraphConvolution (11 -> 1)\n",
       "    )\n",
       "  )\n",
       "  (edgeAttention): Edge_Attention(\n",
       "    (Attention): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "      (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "  (cov1): Conv1d(2, 16, kernel_size=(100,), stride=(100,))\n",
       "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (cov2): Conv1d(16, 32, kernel_size=(4,), stride=(4,))\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (denseLayer1): Linear(in_features=800, out_features=512, bias=True)\n",
       "  (dropout3): Dropout(p=0.5, inplace=False)\n",
       "  (denseLayer2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (dropout4): Dropout(p=0.5, inplace=False)\n",
       "  (outputLayer): Linear(in_features=128, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 60%  k = 67,  80% = 99\n",
    "# classNum,dropout_rate,nfeat,nhid,nclass,n_layers,k,head,features_length\n",
    "model = Classifier(\n",
    "    classNum = 1,\n",
    "    dropout_rate=0.5,\n",
    "    nfeat = 100, \n",
    "    nhid = 11, \n",
    "    nclass = 1, \n",
    "    n_layers = 9, \n",
    "    k = 200,\n",
    "    head = 2,\n",
    "    features_length = 100\n",
    ")\n",
    "sortk = 200\n",
    "batch_size = 32  \n",
    "threshold = 0.45\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "# loss = nn.CrossEntropyLoss()  \n",
    "loss = nn.BCEWithLogitsLoss()    # This loss combines a Sigmoid layer and the BCELoss in one single class.\n",
    "# optimizer = optim.SGD(model.parameters(),lr = 0.001)\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.1)\n",
    "scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.0004 ,cycle_momentum = False)\n",
    "\n",
    "patience = 25 # When the loss of on validation set did not decrease in 20 consecutive training cycles, the model training was stopped to prevent model overfitting\n",
    "early_stopping = EarlyStopping(patience, verbose=True)\n",
    "\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_acc = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "l_rs = []\n",
    "Precesion = []\n",
    "Recall = []\n",
    "F1_score = []\n",
    "AUC = []\n",
    "wf = './BFS_EA_RGCN(SG)/'\n",
    "early_stop_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training():\n",
    "    global train_graph_data,train_edge_data,train_label,early_stop_flag,model\n",
    "    for j in range(500):\n",
    "        # 每个epoch打乱训练集数据\n",
    "        if early_stop_flag:\n",
    "            break\n",
    "        train_graph_data,train_edge_data,train_label = shuffle(train_graph_data,train_edge_data,train_label)\n",
    "        train(j)\n",
    "    # 获得 early stopping 时的模型参数\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.203829).  Saving model ...\n",
      "Validation loss decreased (0.203829 --> 0.177505).  Saving model ...\n",
      "Validation loss decreased (0.177505 --> 0.171084).  Saving model ...\n",
      "Validation loss decreased (0.171084 --> 0.170744).  Saving model ...\n",
      "Validation loss decreased (0.170744 --> 0.166787).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Validation loss decreased (0.166787 --> 0.157121).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Validation loss decreased (0.157121 --> 0.143855).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "EarlyStopping counter: 11 out of 25\n",
      "EarlyStopping counter: 12 out of 25\n",
      "EarlyStopping counter: 13 out of 25\n",
      "EarlyStopping counter: 14 out of 25\n",
      "EarlyStopping counter: 15 out of 25\n",
      "EarlyStopping counter: 16 out of 25\n",
      "EarlyStopping counter: 17 out of 25\n",
      "EarlyStopping counter: 18 out of 25\n",
      "Validation loss decreased (0.143855 --> 0.140622).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Validation loss decreased (0.140622 --> 0.136262).  Saving model ...\n",
      "Validation loss decreased (0.136262 --> 0.136033).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Validation loss decreased (0.136033 --> 0.134494).  Saving model ...\n",
      "Validation loss decreased (0.134494 --> 0.133346).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "EarlyStopping counter: 11 out of 25\n",
      "EarlyStopping counter: 12 out of 25\n",
      "EarlyStopping counter: 13 out of 25\n",
      "Validation loss decreased (0.133346 --> 0.132047).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "Validation loss decreased (0.132047 --> 0.131415).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "EarlyStopping counter: 11 out of 25\n",
      "EarlyStopping counter: 12 out of 25\n",
      "EarlyStopping counter: 13 out of 25\n",
      "EarlyStopping counter: 14 out of 25\n",
      "EarlyStopping counter: 15 out of 25\n",
      "EarlyStopping counter: 16 out of 25\n",
      "EarlyStopping counter: 17 out of 25\n",
      "EarlyStopping counter: 18 out of 25\n",
      "EarlyStopping counter: 19 out of 25\n",
      "EarlyStopping counter: 20 out of 25\n",
      "EarlyStopping counter: 21 out of 25\n",
      "EarlyStopping counter: 22 out of 25\n",
      "EarlyStopping counter: 23 out of 25\n",
      "EarlyStopping counter: 24 out of 25\n",
      "EarlyStopping counter: 25 out of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [51:01<7:39:17, 3061.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp-0 acc: 0.945000  precesion: 0.939252  recall: 0.957143  f1_score: 0.948113 auc: 0.993108 time: 3061.978524\n",
      "Validation loss decreased (inf --> 0.182422).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Validation loss decreased (0.182422 --> 0.178823).  Saving model ...\n",
      "Validation loss decreased (0.178823 --> 0.171028).  Saving model ...\n",
      "Validation loss decreased (0.171028 --> 0.142370).  Saving model ...\n",
      "Validation loss decreased (0.142370 --> 0.133693).  Saving model ...\n",
      "Validation loss decreased (0.133693 --> 0.132945).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Validation loss decreased (0.132945 --> 0.123256).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Validation loss decreased (0.123256 --> 0.120036).  Saving model ...\n",
      "Validation loss decreased (0.120036 --> 0.119418).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Validation loss decreased (0.119418 --> 0.113755).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Validation loss decreased (0.113755 --> 0.113651).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Validation loss decreased (0.113651 --> 0.110832).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Validation loss decreased (0.110832 --> 0.107998).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Validation loss decreased (0.107998 --> 0.104567).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "EarlyStopping counter: 11 out of 25\n",
      "EarlyStopping counter: 12 out of 25\n",
      "EarlyStopping counter: 13 out of 25\n",
      "EarlyStopping counter: 14 out of 25\n",
      "EarlyStopping counter: 15 out of 25\n",
      "EarlyStopping counter: 16 out of 25\n",
      "EarlyStopping counter: 17 out of 25\n",
      "EarlyStopping counter: 18 out of 25\n",
      "EarlyStopping counter: 19 out of 25\n",
      "EarlyStopping counter: 20 out of 25\n",
      "EarlyStopping counter: 21 out of 25\n",
      "EarlyStopping counter: 22 out of 25\n",
      "EarlyStopping counter: 23 out of 25\n",
      "EarlyStopping counter: 24 out of 25\n",
      "EarlyStopping counter: 25 out of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [1:25:50<6:09:20, 2770.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp-1 acc: 0.945000  precesion: 0.970000  recall: 0.923810  f1_score: 0.946341 auc: 0.991429 time: 2088.970290\n",
      "Validation loss decreased (inf --> 0.207452).  Saving model ...\n",
      "Validation loss decreased (0.207452 --> 0.194146).  Saving model ...\n",
      "Validation loss decreased (0.194146 --> 0.172528).  Saving model ...\n",
      "Validation loss decreased (0.172528 --> 0.163988).  Saving model ...\n",
      "Validation loss decreased (0.163988 --> 0.148780).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "Validation loss decreased (0.148780 --> 0.146069).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Validation loss decreased (0.146069 --> 0.141990).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "Validation loss decreased (0.141990 --> 0.140660).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "EarlyStopping counter: 11 out of 25\n",
      "EarlyStopping counter: 12 out of 25\n",
      "EarlyStopping counter: 13 out of 25\n",
      "EarlyStopping counter: 14 out of 25\n",
      "EarlyStopping counter: 15 out of 25\n",
      "EarlyStopping counter: 16 out of 25\n",
      "EarlyStopping counter: 17 out of 25\n",
      "EarlyStopping counter: 18 out of 25\n",
      "EarlyStopping counter: 19 out of 25\n",
      "EarlyStopping counter: 20 out of 25\n",
      "EarlyStopping counter: 21 out of 25\n",
      "EarlyStopping counter: 22 out of 25\n",
      "EarlyStopping counter: 23 out of 25\n",
      "EarlyStopping counter: 24 out of 25\n",
      "EarlyStopping counter: 25 out of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [1:56:38<4:50:54, 2493.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp-2 acc: 0.935000  precesion: 0.950980  recall: 0.923810  f1_score: 0.937198 auc: 0.989574 time: 1848.024447\n",
      "Validation loss decreased (inf --> 0.199400).  Saving model ...\n",
      "Validation loss decreased (0.199400 --> 0.187212).  Saving model ...\n",
      "Validation loss decreased (0.187212 --> 0.171322).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Validation loss decreased (0.171322 --> 0.158270).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Validation loss decreased (0.158270 --> 0.152526).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Validation loss decreased (0.152526 --> 0.147968).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Validation loss decreased (0.147968 --> 0.140392).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "Validation loss decreased (0.140392 --> 0.139089).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "EarlyStopping counter: 11 out of 25\n",
      "EarlyStopping counter: 12 out of 25\n",
      "EarlyStopping counter: 13 out of 25\n",
      "EarlyStopping counter: 14 out of 25\n",
      "EarlyStopping counter: 15 out of 25\n",
      "EarlyStopping counter: 16 out of 25\n",
      "EarlyStopping counter: 17 out of 25\n",
      "EarlyStopping counter: 18 out of 25\n",
      "EarlyStopping counter: 19 out of 25\n",
      "EarlyStopping counter: 20 out of 25\n",
      "EarlyStopping counter: 21 out of 25\n",
      "EarlyStopping counter: 22 out of 25\n",
      "EarlyStopping counter: 23 out of 25\n",
      "EarlyStopping counter: 24 out of 25\n",
      "Validation loss decreased (0.139089 --> 0.138512).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "EarlyStopping counter: 11 out of 25\n",
      "EarlyStopping counter: 12 out of 25\n",
      "EarlyStopping counter: 13 out of 25\n",
      "EarlyStopping counter: 14 out of 25\n",
      "EarlyStopping counter: 15 out of 25\n",
      "EarlyStopping counter: 16 out of 25\n",
      "EarlyStopping counter: 17 out of 25\n",
      "EarlyStopping counter: 18 out of 25\n",
      "EarlyStopping counter: 19 out of 25\n",
      "EarlyStopping counter: 20 out of 25\n",
      "EarlyStopping counter: 21 out of 25\n",
      "EarlyStopping counter: 22 out of 25\n",
      "EarlyStopping counter: 23 out of 25\n",
      "EarlyStopping counter: 24 out of 25\n",
      "EarlyStopping counter: 25 out of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [2:39:12<4:11:09, 2511.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp-3 acc: 0.930000  precesion: 0.911765  recall: 0.948980  f1_score: 0.930000 auc: 0.987695 time: 2553.941466\n",
      "Validation loss decreased (inf --> 0.208892).  Saving model ...\n",
      "Validation loss decreased (0.208892 --> 0.194795).  Saving model ...\n",
      "Validation loss decreased (0.194795 --> 0.177607).  Saving model ...\n",
      "Validation loss decreased (0.177607 --> 0.155232).  Saving model ...\n",
      "Validation loss decreased (0.155232 --> 0.147248).  Saving model ...\n",
      "Validation loss decreased (0.147248 --> 0.143840).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Validation loss decreased (0.143840 --> 0.137769).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Validation loss decreased (0.137769 --> 0.136788).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Validation loss decreased (0.136788 --> 0.130626).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Validation loss decreased (0.130626 --> 0.127206).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "Validation loss decreased (0.127206 --> 0.125616).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "EarlyStopping counter: 11 out of 25\n",
      "EarlyStopping counter: 12 out of 25\n",
      "EarlyStopping counter: 13 out of 25\n",
      "EarlyStopping counter: 14 out of 25\n",
      "EarlyStopping counter: 15 out of 25\n",
      "EarlyStopping counter: 16 out of 25\n",
      "EarlyStopping counter: 17 out of 25\n",
      "EarlyStopping counter: 18 out of 25\n",
      "EarlyStopping counter: 19 out of 25\n",
      "EarlyStopping counter: 20 out of 25\n",
      "EarlyStopping counter: 21 out of 25\n",
      "EarlyStopping counter: 22 out of 25\n",
      "EarlyStopping counter: 23 out of 25\n",
      "EarlyStopping counter: 24 out of 25\n",
      "EarlyStopping counter: 25 out of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [3:12:45<3:16:49, 2361.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp-4 acc: 0.940000  precesion: 0.925743  recall: 0.954082  f1_score: 0.939698 auc: 0.986432 time: 2012.813725\n",
      "Validation loss decreased (inf --> 0.225152).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Validation loss decreased (0.225152 --> 0.199008).  Saving model ...\n",
      "Validation loss decreased (0.199008 --> 0.190255).  Saving model ...\n",
      "Validation loss decreased (0.190255 --> 0.181986).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Validation loss decreased (0.181986 --> 0.166852).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Validation loss decreased (0.166852 --> 0.164183).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Validation loss decreased (0.164183 --> 0.157379).  Saving model ...\n",
      "Validation loss decreased (0.157379 --> 0.153965).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Validation loss decreased (0.153965 --> 0.151583).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "Validation loss decreased (0.151583 --> 0.147263).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "EarlyStopping counter: 11 out of 25\n",
      "EarlyStopping counter: 12 out of 25\n",
      "EarlyStopping counter: 13 out of 25\n",
      "EarlyStopping counter: 14 out of 25\n",
      "EarlyStopping counter: 15 out of 25\n",
      "EarlyStopping counter: 16 out of 25\n",
      "EarlyStopping counter: 17 out of 25\n",
      "EarlyStopping counter: 18 out of 25\n",
      "Validation loss decreased (0.147263 --> 0.147087).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "Validation loss decreased (0.147087 --> 0.139776).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "EarlyStopping counter: 11 out of 25\n",
      "EarlyStopping counter: 12 out of 25\n",
      "EarlyStopping counter: 13 out of 25\n",
      "EarlyStopping counter: 14 out of 25\n",
      "EarlyStopping counter: 15 out of 25\n",
      "EarlyStopping counter: 16 out of 25\n",
      "EarlyStopping counter: 17 out of 25\n",
      "EarlyStopping counter: 18 out of 25\n",
      "EarlyStopping counter: 19 out of 25\n",
      "EarlyStopping counter: 20 out of 25\n",
      "EarlyStopping counter: 21 out of 25\n",
      "EarlyStopping counter: 22 out of 25\n",
      "EarlyStopping counter: 23 out of 25\n",
      "EarlyStopping counter: 24 out of 25\n",
      "EarlyStopping counter: 25 out of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [4:00:04<2:46:59, 2504.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp-5 acc: 0.945000  precesion: 0.966480  recall: 0.915344  f1_score: 0.940217 auc: 0.989393 time: 2838.461866\n",
      "Validation loss decreased (inf --> 0.194047).  Saving model ...\n",
      "Validation loss decreased (0.194047 --> 0.164648).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Validation loss decreased (0.164648 --> 0.135215).  Saving model ...\n",
      "Validation loss decreased (0.135215 --> 0.130379).  Saving model ...\n",
      "Validation loss decreased (0.130379 --> 0.128373).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Validation loss decreased (0.128373 --> 0.127133).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Validation loss decreased (0.127133 --> 0.120255).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Validation loss decreased (0.120255 --> 0.117676).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Validation loss decreased (0.117676 --> 0.114513).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "Validation loss decreased (0.114513 --> 0.113908).  Saving model ...\n",
      "Validation loss decreased (0.113908 --> 0.105970).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "EarlyStopping counter: 11 out of 25\n",
      "EarlyStopping counter: 12 out of 25\n",
      "EarlyStopping counter: 13 out of 25\n",
      "EarlyStopping counter: 14 out of 25\n",
      "EarlyStopping counter: 15 out of 25\n",
      "EarlyStopping counter: 16 out of 25\n",
      "EarlyStopping counter: 17 out of 25\n",
      "EarlyStopping counter: 18 out of 25\n",
      "EarlyStopping counter: 19 out of 25\n",
      "EarlyStopping counter: 20 out of 25\n",
      "EarlyStopping counter: 21 out of 25\n",
      "EarlyStopping counter: 22 out of 25\n",
      "EarlyStopping counter: 23 out of 25\n",
      "EarlyStopping counter: 24 out of 25\n",
      "EarlyStopping counter: 25 out of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [4:30:51<1:55:22, 2307.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp-6 acc: 0.930000  precesion: 0.948454  recall: 0.910891  f1_score: 0.929293 auc: 0.986874 time: 1846.992035\n",
      "Validation loss decreased (inf --> 0.205835).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Validation loss decreased (0.205835 --> 0.174513).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Validation loss decreased (0.174513 --> 0.164062).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Validation loss decreased (0.164062 --> 0.155202).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Validation loss decreased (0.155202 --> 0.152937).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Validation loss decreased (0.152937 --> 0.149310).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "EarlyStopping counter: 11 out of 25\n",
      "EarlyStopping counter: 12 out of 25\n",
      "EarlyStopping counter: 13 out of 25\n",
      "EarlyStopping counter: 14 out of 25\n",
      "EarlyStopping counter: 15 out of 25\n",
      "EarlyStopping counter: 16 out of 25\n",
      "EarlyStopping counter: 17 out of 25\n",
      "EarlyStopping counter: 18 out of 25\n",
      "EarlyStopping counter: 19 out of 25\n",
      "EarlyStopping counter: 20 out of 25\n",
      "EarlyStopping counter: 21 out of 25\n",
      "EarlyStopping counter: 22 out of 25\n",
      "EarlyStopping counter: 23 out of 25\n",
      "EarlyStopping counter: 24 out of 25\n",
      "EarlyStopping counter: 25 out of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [4:54:10<1:07:50, 2035.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp-7 acc: 0.917500  precesion: 0.886878  recall: 0.960784  f1_score: 0.922353 auc: 0.979154 time: 1399.287441\n",
      "Validation loss decreased (inf --> 0.159219).  Saving model ...\n",
      "Validation loss decreased (0.159219 --> 0.149218).  Saving model ...\n",
      "Validation loss decreased (0.149218 --> 0.138789).  Saving model ...\n",
      "Validation loss decreased (0.138789 --> 0.124273).  Saving model ...\n",
      "Validation loss decreased (0.124273 --> 0.108924).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Validation loss decreased (0.108924 --> 0.107031).  Saving model ...\n",
      "Validation loss decreased (0.107031 --> 0.099982).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "Validation loss decreased (0.099982 --> 0.097381).  Saving model ...\n",
      "Validation loss decreased (0.097381 --> 0.090306).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "EarlyStopping counter: 11 out of 25\n",
      "EarlyStopping counter: 12 out of 25\n",
      "EarlyStopping counter: 13 out of 25\n",
      "EarlyStopping counter: 14 out of 25\n",
      "EarlyStopping counter: 15 out of 25\n",
      "EarlyStopping counter: 16 out of 25\n",
      "EarlyStopping counter: 17 out of 25\n",
      "EarlyStopping counter: 18 out of 25\n",
      "EarlyStopping counter: 19 out of 25\n",
      "EarlyStopping counter: 20 out of 25\n",
      "EarlyStopping counter: 21 out of 25\n",
      "EarlyStopping counter: 22 out of 25\n",
      "EarlyStopping counter: 23 out of 25\n",
      "EarlyStopping counter: 24 out of 25\n",
      "EarlyStopping counter: 25 out of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [5:22:02<32:06, 1926.27s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp-8 acc: 0.932500  precesion: 0.949749  recall: 0.917476  f1_score: 0.933333 auc: 0.985312 time: 1672.402855\n",
      "Validation loss decreased (inf --> 0.172137).  Saving model ...\n",
      "Validation loss decreased (0.172137 --> 0.160355).  Saving model ...\n",
      "Validation loss decreased (0.160355 --> 0.147182).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Validation loss decreased (0.147182 --> 0.146316).  Saving model ...\n",
      "Validation loss decreased (0.146316 --> 0.136702).  Saving model ...\n",
      "Validation loss decreased (0.136702 --> 0.133493).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Validation loss decreased (0.133493 --> 0.122487).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Validation loss decreased (0.122487 --> 0.120990).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "Validation loss decreased (0.120990 --> 0.116825).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "EarlyStopping counter: 3 out of 25\n",
      "EarlyStopping counter: 4 out of 25\n",
      "EarlyStopping counter: 5 out of 25\n",
      "EarlyStopping counter: 6 out of 25\n",
      "EarlyStopping counter: 7 out of 25\n",
      "EarlyStopping counter: 8 out of 25\n",
      "EarlyStopping counter: 9 out of 25\n",
      "EarlyStopping counter: 10 out of 25\n",
      "EarlyStopping counter: 11 out of 25\n",
      "EarlyStopping counter: 12 out of 25\n",
      "EarlyStopping counter: 13 out of 25\n",
      "EarlyStopping counter: 14 out of 25\n",
      "EarlyStopping counter: 15 out of 25\n",
      "EarlyStopping counter: 16 out of 25\n",
      "EarlyStopping counter: 17 out of 25\n",
      "EarlyStopping counter: 18 out of 25\n",
      "EarlyStopping counter: 19 out of 25\n",
      "EarlyStopping counter: 20 out of 25\n",
      "EarlyStopping counter: 21 out of 25\n",
      "EarlyStopping counter: 22 out of 25\n",
      "EarlyStopping counter: 23 out of 25\n",
      "EarlyStopping counter: 24 out of 25\n",
      "EarlyStopping counter: 25 out of 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [5:51:36<00:00, 2109.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp-9 acc: 0.917500  precesion: 0.898990  recall: 0.931937  f1_score: 0.915167 auc: 0.986347 time: 1773.305332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(30)):\n",
    "    s_time = time()\n",
    "    # 打乱数据\n",
    "    graph_filename,edge_filename,all_label = shuffle(graph_filename,edge_filename,all_label)\n",
    "\n",
    "    #划分数据集\n",
    "    train_graph_data = graph_filename[0:int(k*length)]\n",
    "    train_label = all_label[0:int(k*length)]\n",
    "    train_edge_data = edge_filename[0:int(k*length)]\n",
    "\n",
    "    valid_graph_data = graph_filename[int(k*length):int(k1*length)]\n",
    "    valid_label = all_label[int(k*length):int(k1*length)]\n",
    "    valid_edge_data = edge_filename[int(k*length):int(k1*length)]\n",
    "\n",
    "    test_graph_data = graph_filename[int(k1*length):]\n",
    "    test_label = all_label[int(k1*length):]\n",
    "    test_edge_data = edge_filename[int(k1*length):]\n",
    "    \n",
    "    model = Classifier(\n",
    "    classNum = 1,\n",
    "    dropout_rate=0.5,\n",
    "    nfeat = 100, \n",
    "    nhid = 11, \n",
    "    nclass = 1, \n",
    "    n_layers = 9, \n",
    "    k = 200,\n",
    "    head = 2,\n",
    "    features_length = 100\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    # loss = nn.CrossEntropyLoss()  \n",
    "    loss = nn.BCEWithLogitsLoss()    # This loss combines a Sigmoid layer and the BCELoss in one single class.\n",
    "    # optimizer = optim.SGD(model.parameters(),lr = 0.001)\n",
    "    optimizer = optim.Adam(model.parameters(),lr = 0.1)\n",
    "    scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.0004 ,cycle_momentum = False)\n",
    "    early_stopping = EarlyStopping(patience, verbose=True) # 关于 EarlyStopping 的代码可先看博客后面的内容\n",
    "    early_stop_flag = False\n",
    "    start_training()\n",
    "    \n",
    "    pre,recall,f1,acc,auc = test()\n",
    "    e_time = time()\n",
    "    f = open(wf+'test_result.txt','a')\n",
    "    print('exp-%d acc: %f  precesion: %f  recall: %f  f1_score: %f auc: %f\\n' % (i,acc, pre, recall, f1,auc),file = f)\n",
    "    print('exp-%d acc: %f  precesion: %f  recall: %f  f1_score: %f auc: %f time: %f' % (i,acc, pre, recall, f1,auc, e_time - s_time))\n",
    "    test_acc.append(acc)\n",
    "    Precesion.append(pre)\n",
    "    Recall.append(recall)\n",
    "    F1_score.append(f1)\n",
    "    AUC.append(auc)\n",
    "    f.close()\n",
    "    \n",
    "f = open(wf+'test_result.txt','a')\n",
    "print('ave_acc: %f  ave_precesion: %f  ave_recall: %f  ave_f1_score: %f ave_auc: %f\\n' % (np.array(test_acc).mean(), np.array(Precesion).mean(), np.array(Recall).mean(), np.array(F1_score).mean(),np.array(AUC).mean()),file=f)\n",
    "print('var_acc: %f  var_precesion: %f  var_recall: %f  var_f1_score: %f var_auc: %f\\n' % (np.array(test_acc).var(), np.array(Precesion).var(), np.array(Recall).var(), np.array(F1_score).var(),np.array(AUC).var()),file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, parms in model.named_parameters():\t\n",
    "#     print('-->name:', name, '-->grad_requirs:',parms.requires_grad, \\\n",
    "#              ' -->grad_value:',parms.grad)    #查看梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, wf+'BFS_EA_RGCN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.loadtxt('./BFS_EA_RGCN/500_bat5_roc_curve.txt')\n",
    "# b = np.loadtxt('./DFS_EA_RGCN/500_roc_curve.txt')\n",
    "# c = np.loadtxt('./GCN/500_roc_curve.txt')\n",
    "# d = np.loadtxt('./BFS_EA_GCN/500_roc_curve.txt')\n",
    "\n",
    "# plt.title('Roc Curve')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.xlim([-0.05,1])\n",
    "# plt.ylim([-0.05,1.05])\n",
    "# plt.plot(a[0],a[1],'b-')\n",
    "# plt.plot(b[0],b[1],'g-')\n",
    "# plt.plot(c[0],c[1],'r-')\n",
    "# plt.plot(d[0],d[1],'c-')\n",
    "\n",
    "# plt.legend(['BFS_EA_RGCN | auc=0.9344','DFS_EA_RGCN | auc=0.9188','GCN | auc=0.8886','BFS_EA_GCN | auc = 0.9296'])\n",
    "\n",
    "# plt.savefig(wf+'cmp_roc.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
